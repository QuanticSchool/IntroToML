{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB3sNglQrnTN"
      },
      "source": [
        "Data comes from [The Extrasolar Planet Encyclopedia](http://exoplanet.eu/). Thanks to Ilya Marchenko for sharing this dataset on [Kaggle](https://www.kaggle.com/ilyamarchenko/full-exoplanet-catalog?select=exoplanet_confirm_and_candidates.csv)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2AaGqEU9bC1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BI5pE0bLfsB"
      },
      "outputs": [],
      "source": [
        "exo_full_dataset = pd.read_csv('/content/drive/My Drive/exoplanets.csv')\n",
        "exo_full_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97Titj3M_tmn"
      },
      "outputs": [],
      "source": [
        "exo = exo_full_dataset.loc[:, ['radius', 'mass', 'planet_status', 'orbital_period', 'star_distance']] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jfak73vWDrX_"
      },
      "outputs": [],
      "source": [
        "print(\"\\nUnique values\\n\",exo.nunique())\n",
        "print(\"\\nNull values\\n\\n\", exo.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKwGAw9-NQqS"
      },
      "source": [
        "## Create dummy example data\n",
        "\n",
        "For all techniques we'll first demonstrate them on the simple DataFrame created below, then on the more realistic data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCJ63eqvIbwq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "alien_species = {\"alien_height\":[80, 63, 70, 93, np.nan], \"alien_age\":[12, np.nan, 87, 415, 892], \"home_planet\":[\"Mars\", \"Jupiter\", \"Europa\", \"Mars\", \"Europa\"]}\n",
        "\n",
        "alien_df = pd.DataFrame(alien_species)\n",
        "alien_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJwieNn9ISaA"
      },
      "source": [
        "## Imputation\n",
        "First the simple DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng-iUCzzqj4d"
      },
      "source": [
        "### `alien_df` Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJPkQ-KoNel2"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "features = alien_df.loc[:, [\"alien_height\", \"alien_age\"]]\n",
        "print(features.head(), \"\\n\")\n",
        "imp = SimpleImputer()\n",
        "imp.fit(features)\n",
        "imputed = imp.transform(features)\n",
        "\n",
        "# the rest of this code block reformats the data to print it in an educative way. don't sweat it!\n",
        "# scikit learn often strips the column headers (it's due to converting arrays to numpy for math), so add them back like so:\n",
        "imputed_alien_df = pd.DataFrame(imputed,columns=features.columns)\n",
        "print(imputed_alien_df.head())\n",
        "# adding back the categorical data\n",
        "imputed_alien_df[\"home_planet\"] = alien_df[\"home_planet\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLF_NofQNx7u"
      },
      "source": [
        "Now let's perform imputation on the exoplanets dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfP5unpVqpAq"
      },
      "source": [
        "### Exoplanets Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esqekNcEN7PB"
      },
      "outputs": [],
      "source": [
        "exo_numbers = exo.loc[:, ['radius', 'mass', 'orbital_period', 'star_distance']]\n",
        "print(exo.head())\n",
        "print(\"\\nNull values\\n\\n\", exo.isna().sum(), \"\\n\")\n",
        "imp = SimpleImputer()\n",
        "imp.fit(exo_numbers)\n",
        "imputed = imp.transform(exo_numbers)\n",
        "imputed_exo_df = pd.DataFrame(imputed,columns=exo_numbers.columns)\n",
        "\n",
        "# reformatting the imputed data below\n",
        "imputed_exo_df = pd.DataFrame(imputed,columns=exo_numbers.columns)\n",
        "# adding back in the categorical data\n",
        "imputed_exo_df[\"planet_status\"] = exo[\"planet_status\"]\n",
        "print(imputed_exo_df.head())\n",
        "print(\"\\nNull values\\n\\n\", imputed_exo_df.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51Xkz1fUIcJB"
      },
      "source": [
        "## One-Hot Encoding\n",
        "First the `alien_df` data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wkt0SNm7qzks"
      },
      "source": [
        "### `alien_df` Example"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_alien_df = pd.get_dummies(what_goes_here?)\n",
        "print(imputed_alien_df, \"\\n\")\n",
        "print(enc_alien_df)"
      ],
      "metadata": {
        "id": "wvzsL3VP3NTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D_8S4pEo6_4"
      },
      "source": [
        "Now the exoplanet dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z1qGJ8Hq1cG"
      },
      "source": [
        "### Exoplanets Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHQKMeJ8F6nZ"
      },
      "outputs": [],
      "source": [
        "print(imputed_exo_df.loc[:, \"planet_status\"].unique())\n",
        "imputed_exo_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_exo_df = what_goes_here?\n",
        "print(imputed_exo_df.head(), \"\\n\")\n",
        "print(enc_exo_df)"
      ],
      "metadata": {
        "id": "lQDgcWv24_G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0eXqDNombOX"
      },
      "source": [
        "## Further Practice\n",
        "If you want to sharpen your skills, try feature scaling the exoplanet data below. Then you'd have a wholly preprocessed dataset!\n",
        "\n",
        "(Since the one-hot encoded data occurs within the normal range of standard deviation, you don't need to worry about scaling it.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuipIATXnIRT"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# remember how this goes?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Encoding-Only Learner Version.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}