{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Scaling",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Our Unscaled Data\n",
        "\n",
        "We'll start by creating a small DataFrame with two imbalanced features and a label."
      ],
      "metadata": {
        "id": "DWGTYTpp2YWr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJHFZpykoCmO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "alien_fuel_data = {\"alien_weight\":[80, 63, 70, 93, 67, 72, 88, 61], \n",
        "              \"spaceship_weight\":[2993, 3267, 4231, 3987, 2324, 4118, 5003, 2576], \n",
        "              \"fuel\":[523, 353, 489, 628, 411, 528, 339, 418]}\n",
        "\n",
        "# practice what we learned last lesson\n",
        "alien_fuel_df = \"turn alien_fuel_data into a dataframe\"\n",
        "alien_fuel_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# practice more from last lesson and extract the features and labels into X and y\n",
        "X = \"extract alien_weight and spaceship_weight,  features\"\n",
        "y = \"extract fuel, the label\""
      ],
      "metadata": {
        "id": "Au5-W-CUf-QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Revisiting the `train_test_split`\n",
        "\n",
        "Remember how we do this?\n",
        "\n",
        "If not, no biggie. That's what [docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) are for."
      ],
      "metadata": {
        "id": "gKBmIhLrfHH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training data: \\n\", X_train, \"\\n\")\n",
        "print(\"Test data: \\n\", X_test)"
      ],
      "metadata": {
        "id": "gC3PasGqfaWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling the training and test data\n",
        "\n",
        "To understand how we use the training data's values to scale the testing data, we'll need to explore SciKit Learn's `.fit()` and `.transform()` methods.\n",
        "\n",
        "We first obtain the mean and standard deviation from the training data."
      ],
      "metadata": {
        "id": "FkEO6zXQ2jRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "scaler = preprocessing.StandardScaler().fit(\"what should you put here?\") # creates the standardization values\n",
        "\n",
        "print(scaler.mean_) # prints the mean of both columns of training data\n",
        "print(scaler.scale_) # prints the SD of both columns of training data"
      ],
      "metadata": {
        "id": "mOpZtEgithdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transforming the training data"
      ],
      "metadata": {
        "id": "oF1r_2K5qlGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we use the values generated by `.fit()` to transform the training data."
      ],
      "metadata": {
        "id": "aBKFLaqhsje7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled = scaler.transform(X_train) # transforms the training data based on the values generated from .fit()\n",
        "\n",
        "print(\"Training data before scaling: \\n\", X_train, \"\\n\")\n",
        "\n",
        "# SKL often strips the column headers after transformation. They're added back on the following line.\n",
        "X_train_scaled_formatted = pd.DataFrame(X_train_scaled,columns=X_train.columns) \n",
        "print(\"Training data after scaling: \\n\", X_train_scaled_formatted)"
      ],
      "metadata": {
        "id": "j0mt9nf5oA4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transforming the testing set\n",
        "\n",
        "Then we use the same values from fitting the training data to transform the testing data. This prevents the data leakage that would otherwise occur."
      ],
      "metadata": {
        "id": "kgaRdGilntnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the scaler object still has the same mean and standard deviation that it did on the training set\n",
        "print(scaler.mean_)\n",
        "print(scaler.scale_)\n",
        "\n",
        "# that same transformation is applied to X_test\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"\\nTest data before scaling: \\n\", X_test, \"\\n\")\n",
        "X_test_scaled_formatted = pd.DataFrame(X_test_scaled,columns=X_test.columns)\n",
        "print(\"Test data after scaling: \\n\", X_test_scaled_formatted)"
      ],
      "metadata": {
        "id": "PvrqCKCfmnQY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}